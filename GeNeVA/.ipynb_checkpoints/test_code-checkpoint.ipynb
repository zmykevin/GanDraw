{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmykevin/CoDraw_Gaugan/code/GeNeVA/geneva/utils/config.py:15: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  keys = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import json # Added to initialize the setting in Jupyter Notebook-by Mingyang\n",
    "import easydict # Added to initialize the setting in Jupyter Notebook-by Mingyang\n",
    "\n",
    "from geneva.data.datasets import DATASETS\n",
    "from geneva.evaluation.evaluate import Evaluator\n",
    "from geneva.utils.config import keys, parse_config\n",
    "from geneva.utils.visualize import VisdomPlotter\n",
    "from geneva.models.models import MODELS\n",
    "from geneva.data import codraw_dataset\n",
    "from geneva.data import clevr_dataset\n",
    "\n",
    "import h5py\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"example_args/gandraw_args.json\"\n",
    "#Load the config_file\n",
    "with open(config_file, 'r') as f:\n",
    "    cfg = json.load(f)\n",
    "#convert cfg as easydict\n",
    "cfg = easydict.EasyDict(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset[0]['turns'])\n",
    "#hdf5_dataset = dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "def _parse_glove(glove_path):\n",
    "    glove = {}\n",
    "    with open(glove_path, 'r') as f:\n",
    "        for line in f:\n",
    "            splitline = line.split()\n",
    "            word = splitline[0]\n",
    "            embedding = np.array([float(val) for val in splitline[1:]])\n",
    "            glove[word] = embedding\n",
    "\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "    \n",
    "class GanDrawDataset(nn.Module):\n",
    "\n",
    "    def __init__(self, path, cfg, img_size=128, glove_path=None):\n",
    "        super(GanDrawDataset, self).__init__()\n",
    "        self.dataset = None\n",
    "        self.dataset_path = path\n",
    "\n",
    "        self.glove = _parse_glove(keys['glove_gandraw_path'])\n",
    "        # update cfg\n",
    "        #cfg.vocab_size = len(self.glove.keys())\n",
    "\n",
    "        self.keys = []\n",
    "        with h5py.File(path, 'r') as f:\n",
    "            # print(len(list(f.keys())))\n",
    "            for i in range(len(list(f.keys())) - 1):\n",
    "                #assert f[str(i)]['objects'].shape[0] == f[str(i)]['utterences'].shape[0]\n",
    "                self.keys.append(f[str(i)]['utterences'].shape[0])\n",
    "\n",
    "        self.keys = np.argsort(np.array(self.keys))[::-1]\n",
    "        self.blocks_maps = {}\n",
    "        for i in range(0, len(self.keys) - 1, cfg.batch_size):\n",
    "            block_key = i // cfg.batch_size\n",
    "            self.blocks_maps[block_key] = self.keys[i:i + cfg.batch_size]\n",
    "\n",
    "        self.blocks_keys = np.array(list(self.blocks_maps.keys()))\n",
    "\n",
    "        # Load the Vocab from the Path\n",
    "        gandraw_vocab_path = cfg.gandraw_vocab_path\n",
    "        with open(gandraw_vocab_path, 'r') as f:\n",
    "            gandraw_vocab = f.readlines()\n",
    "            gandraw_vocab = [x.strip().rsplit(' ', 1)[0]\n",
    "                             for x in gandraw_vocab]\n",
    "\n",
    "        # print(len(gandraw_vocab))\n",
    "        self.vocab = ['<s_start>', '<s_end>', '<unk>',\n",
    "                      '<pad>', '<d_end>'] + gandraw_vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        # update the vocab_size\n",
    "        #cfg.vocab_size = len(self.vocab_size)\n",
    "\n",
    "        # format word2ind ind2word\n",
    "        self.word2index = {k: v for v, k in enumerate(self.vocab)}\n",
    "        self.index2word = {v: k for v, k in enumerate(self.vocab)}\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.image_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        self.unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(\n",
    "                                 0.229, 0.224, 0.225))\n",
    "        \n",
    "        # Preprocess the Raw Back Ground Images\n",
    "        self.background = cv2.imread(cfg.gandraw_background)\n",
    "        self.background = cv2.cvtColor(self.background, cv2.COLOR_BGR2RGB)\n",
    "        self.background = np.expand_dims(self.background, axis=0)\n",
    "        #print(self.background.shape)\n",
    "        self.background = self.process_image(self.background)\n",
    "        \n",
    "        self.gandraw_entities = {\n",
    "            156: {\"name\": \"sky\", \"index\": 0},\n",
    "            110: {\"name\": \"dirt\", \"index\": 1},\n",
    "            124: {\"name\": \"gravel\", \"index\": 2},\n",
    "            135: {\"name\": \"mud\", \"index\": 3},\n",
    "            14: {\"name\": \"sand\", \"index\": 4},\n",
    "            105: {\"name\": \"clouds\", \"index\": 5},\n",
    "            119: {\"name\": \"fog\", \"index\": 6},\n",
    "            126: {\"name\": \"hill\", \"index\": 7},\n",
    "            134: {\"name\": \"mountain\", \"index\": 8},\n",
    "            147: {\"name\": \"river\", \"index\": 9},\n",
    "            149: {\"name\": \"rock\", \"index\": 10},\n",
    "            154: {\"name\": \"sea\", \"index\": 11},\n",
    "            158: {\"name\": \"snow\", \"index\": 12},\n",
    "            161: {\"name\": \"stone\", \"index\": 13},\n",
    "            177: {\"name\": \"water\", \"index\": 14},\n",
    "            96: {\"name\": \"bush\", \"index\": 15},\n",
    "            118: {\"name\": \"flower\", \"index\": 16},\n",
    "            123: {\"name\": \"grass\", \"index\": 17},\n",
    "            162: {\"name\": \"straw\", \"index\": 18},\n",
    "            168: {\"name\": \"tree\", \"index\": 19},\n",
    "            181: {\"name\": \"wood\", \"index\": 20}\n",
    "                                }\n",
    "        self.gandraw_entities_len = cfg.num_objects\n",
    "        \n",
    "    def __len__(self):\n",
    "        with h5py.File(self.dataset_path, 'r') as f:\n",
    "            return len(list(f.keys())) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.dataset is None:\n",
    "            self.dataset = h5py.File(self.dataset_path, 'r')\n",
    "\n",
    "        block_index = self.blocks_keys[idx // self.cfg.batch_size]\n",
    "        sample_index = idx % self.cfg.batch_size\n",
    "\n",
    "        if sample_index > len(self.blocks_maps[block_index]) - 1:\n",
    "            sample_index = len(self.blocks_maps[block_index]) - 1\n",
    "\n",
    "        example = self.dataset[\n",
    "            str(self.blocks_maps[block_index][sample_index])]\n",
    "        images = example['images'].value\n",
    "        images_semantic = example['images_semantic'].value #Added for counting objects\n",
    "        \n",
    "        turns = example['utterences'].value\n",
    "        print(turns.shape)\n",
    "        scene_id = example['scene_id'].value\n",
    "        target_images = example['target_images'].value\n",
    "        target_images_segmentation = example[\n",
    "            'target_images_segmentation'].value\n",
    "        target_images_path = example['target_images_path'].value\n",
    "#         objects = example['objects'].value\n",
    "#         scene_id = example['scene_id'].value\n",
    "\n",
    "        turns_tokenized = [t.split() for t in turns]\n",
    "        lengths = [len(t) for t in turns_tokenized]\n",
    "\n",
    "        turns_word_embeddings = np.zeros((len(turns), max(lengths), 300))\n",
    "\n",
    "        for i, turn in enumerate(turns_tokenized):\n",
    "            for j, w in enumerate(turn):\n",
    "                turns_word_embeddings[i, j] = self.glove[w]\n",
    "\n",
    "        # Process Images\n",
    "        images = self.process_image(images)\n",
    "\n",
    "        # Process Target Images\n",
    "        target_images = self.process_image(target_images)\n",
    "        #######################################################################\n",
    "\n",
    "        ###################Extract the Teller Turn Index and Drawer Turn Index#\n",
    "        teller_turn_ids, drawer_turn_ids, teller_drawer_turn_ids = self.separate_drawer_teller(\n",
    "            turns_tokenized)\n",
    "        teller_id_lengths = [len(t) for t in teller_turn_ids]\n",
    "        drawer_id_lengths = [len(t) for t in drawer_turn_ids]\n",
    "        teller_drawer_id_lengths = [len(t) for t in teller_drawer_turn_ids]\n",
    "        \n",
    "        ##################Extract the objects###################################\n",
    "        objects = np.zeros((images_semantic.shape[0], self.gandraw_entities_len))\n",
    "        for j in range(images_semantic.shape[0]):\n",
    "            current_semantic = images_semantic[j]\n",
    "            unique_labels = list(np.unique(current_semantic))\n",
    "            #print(unique_labels)\n",
    "            for l in unique_labels:\n",
    "                if self.gandraw_entities.get(l, None) is not None:\n",
    "                    objects[j][self.gandraw_entities[l][\"index\"]] = 1\n",
    "            #print(self.objects[j])\n",
    "        ########################################################################\n",
    "        sample = {\n",
    "            'scene_id': scene_id,\n",
    "            'image': images,\n",
    "            'turns': turns,\n",
    "            'objects': objects,\n",
    "            'turns_word_embedding': turns_word_embeddings,\n",
    "            'turn_lengths': lengths,\n",
    "            'background': self.background,\n",
    "            # Added by Mingyang Zhou\n",
    "            'target_image': target_images,\n",
    "            'target_image_segmentation': target_images_segmentation,\n",
    "            'target_image_path': target_images_path,\n",
    "            'teller_turn_ids': teller_turn_ids,\n",
    "            'drawer_turn_ids': drawer_turn_ids,\n",
    "            'teller_drawer_turn_ids': teller_drawer_turn_ids,\n",
    "            'teller_id_lengths': teller_id_lengths,\n",
    "            'drawer_id_lengths': drawer_id_lengths,\n",
    "            'teller_drawer_id_lengths': teller_drawer_id_lengths\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def process_image(self, images):\n",
    "        result_images = np.zeros_like(images.transpose(0, 3, 1, 2),dtype=np.float32)\n",
    "        #print(result_images.dtype)\n",
    "        for i in range(images.shape[0]):\n",
    "            current_img = images[i]\n",
    "            current_processed_img = self.image_transform(current_img)\n",
    "            current_processed_img = current_processed_img.numpy()\n",
    "            #print(current_processed_img.dtype)\n",
    "            result_images[i] = current_processed_img\n",
    "\n",
    "        return result_images\n",
    "\n",
    "    def separate_drawer_teller(self, turns_tokenized):\n",
    "        \"\"\"\n",
    "        return two list: \n",
    "        1. one list contain the list of list of index for teller\n",
    "        2. one list contain the lsit of list of index for drawer\n",
    "        teller_turns_index have one more round dialogs than the actual data which include an end token in the end.\n",
    "        \"\"\"\n",
    "        teller_turns_index = []  # initialize with a sentence start token\n",
    "        drawer_turns_index = []  # initialize with a sentence start token\n",
    "        teller_drawer_turns_index = []\n",
    "        for t in turns_tokenized:\n",
    "            teller_i_turn = [0]\n",
    "            drawer_i_turn = [0]\n",
    "            teller_drawer_i_turn = [0]\n",
    "            current_role = \"teller\"\n",
    "            for x in t:\n",
    "                if x == \"<teller>\":\n",
    "                    current_role = \"teller\"\n",
    "                elif x == \"<drawer>\":\n",
    "                    current_role = \"drawer\"\n",
    "                else:\n",
    "                    if current_role == \"teller\":\n",
    "                        teller_i_turn.append(self.word2index[x])\n",
    "                    else:\n",
    "                        drawer_i_turn.append(self.word2index[x])\n",
    "                    # Concatenate dialog history into one list\n",
    "                    teller_drawer_i_turn.append(self.word2index[x])\n",
    "\n",
    "            teller_i_turn.append(1)  # Add an end token\n",
    "            drawer_i_turn.append(1)  # Add an end token in the end\n",
    "            teller_drawer_i_turn.append(1)  # Add an end token in the end\n",
    "\n",
    "            teller_turns_index.append(teller_i_turn)\n",
    "            drawer_turns_index.append(drawer_i_turn)\n",
    "            teller_drawer_turns_index.append(teller_drawer_i_turn)\n",
    "        teller_turns_index.append([0, 4, 1])\n",
    "        assert len(teller_turns_index) - \\\n",
    "            len(drawer_turns_index) == 1, \"The teller has one additional turn than drawer\"\n",
    "        return teller_turns_index, drawer_turns_index, teller_drawer_turns_index\n",
    "\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.blocks_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize codrawDialog_dataset\n",
    "gandraw_dataset = GanDrawDataset(path=keys[cfg.dataset], cfg=cfg, img_size=cfg.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "(23,)\n",
      "(23, 3, 128, 128)\n",
      "(14,)\n",
      "(14, 3, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# a = gandraw_dataset[2]\n",
    "# for key, item in a.items():\n",
    "#     if type(item) == np.ndarray:\n",
    "#         print(key)\n",
    "#         print(item.shape)\n",
    "print(len(gandraw_dataset))\n",
    "sample_1 = gandraw_dataset[1]\n",
    "print(sample_1['image'].shape)\n",
    "sample_2 = gandraw_dataset[10]\n",
    "print(sample_2['image'].shape)\n",
    "        \n",
    "    \n",
    "        \n",
    "# print(a[\"scene_id\"])\n",
    "\n",
    "# A = a[\"target_image\"][0]\n",
    "# print(A.shape)\n",
    "\n",
    "# sample_images = torch.FloatTensor(A)\n",
    "# #                 print(sample_images.size())\n",
    "# image = gandraw_dataset.unorm(sample_images)\n",
    "# image = transforms.ToPILImage()(image).convert('RGB')\n",
    "# #                 #image = np.array(image)\n",
    "# plt.imshow(image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gandraw_collate_data(batch):\n",
    "    batch = sorted(batch, key=lambda x: len(x['image']), reverse=True)\n",
    "    dialog_lengths = list(map(lambda x: len(x['image']), batch))\n",
    "    max_len = max(dialog_lengths)\n",
    "    print(max_len)\n",
    "\n",
    "    batch_size = len(batch)\n",
    "    _, c, h, w = batch[0]['image'].shape\n",
    "\n",
    "    batch_longest_turns = [max(b['turn_lengths']) for b in batch]\n",
    "    longest_turn = max(batch_longest_turns)\n",
    "\n",
    "    stacked_images = np.zeros((batch_size, max_len, c, h, w))\n",
    "    stacked_turns = np.zeros((batch_size, max_len, longest_turn, 300)) #300 is the word2vec dimension\n",
    "    stacked_turn_lengths = np.zeros((batch_size, max_len))\n",
    "    stacked_objects = np.zeros((batch_size, max_len, 21))\n",
    "    turns_text = []\n",
    "    scene_ids = []\n",
    "    \n",
    "    #Add the additional stacked information\n",
    "    stacked_target_images = np.zeros((batch_size, 1, c, h, w))\n",
    "    \n",
    "    batch_longest_teller_ids = [max(b['teller_id_lengths']) for b in batch]\n",
    "    longest_teller_id_length = max(batch_longest_teller_ids)\n",
    "    #dialog_id = batch_longest_teller_ids.index(longest_teller_id_length)\n",
    "    batch_longest_drawer_ids = [max(b['drawer_id_lengths']) for b in batch]\n",
    "    longest_drawer_id_length = max(batch_longest_drawer_ids)\n",
    "    batch_longest_teller_drawer_ids = [\n",
    "        max(b['teller_drawer_id_lengths']) for b in batch]\n",
    "    longest_teller_drawer_id_length = max(batch_longest_teller_drawer_ids)\n",
    "    \n",
    "    stacked_teller_turn_ids = np.ones((batch_size, max_len+1, longest_teller_id_length))*3 #3 is the id  of <pad>\n",
    "    stacked_drawer_turn_ids = np.ones((batch_size, max_len, longest_drawer_id_length))*3 #3 is the id of <pad>\n",
    "    stacked_teller_drawer_turn_ids = np.ones(\n",
    "        (batch_size, max_len, longest_teller_drawer_id_length)) * 3\n",
    "    \n",
    "    stacked_teller_turn_ids_lengths = np.zeros((batch_size, max_len+1))\n",
    "    stacked_drawer_turn_ids_lengths = np.zeros((batch_size, max_len))\n",
    "    stacked_teller_drawer_turn_ids_lengths = np.zeros((batch_size, max_len))\n",
    "    \n",
    "    background = None\n",
    "    for i, b in enumerate(batch):\n",
    "        img = b['image']\n",
    "        turns = b['turns']\n",
    "        background = b['background']\n",
    "        turns_word_embedding = b['turns_word_embedding']\n",
    "        turns_lengths = b['turn_lengths']\n",
    "        \n",
    "        #Add the additional information\n",
    "        target_image = b[\"target_image\"]\n",
    "        target_image_segmentation = b[\"target_image_segmentation\"]\n",
    "        target_image_path= b[\"target_image_path\"]\n",
    "        \n",
    "        teller_turn_ids = b[\"teller_turn_ids\"]\n",
    "        drawer_turn_ids = b[\"drawer_turn_ids\"]\n",
    "        teller_drawer_turn_ids = b[\"teller_drawer_turn_ids\"]\n",
    "        \n",
    "        teller_id_lengths = b[\"teller_id_lengths\"]\n",
    "        drawer_id_lengths = b[\"drawer_id_lengths\"]\n",
    "        teller_drawer_id_lengths = b[\"teller_drawer_id_lengths\"]\n",
    "        \n",
    "        \n",
    "        dialog_length = img.shape[0]\n",
    "        stacked_images[i, :dialog_length] = img\n",
    "        stacked_turn_lengths[i, :dialog_length] = np.array(turns_lengths)\n",
    "        stacked_objects[i, :dialog_length] = b['objects']\n",
    "        turns_text.append(turns)\n",
    "        scene_ids.append(b['scene_id'])\n",
    "        \n",
    "        #Update the stacked additional information\n",
    "        stacked_target_images[i] = target_image\n",
    "        stacked_teller_turn_ids_lengths[i, :len(teller_id_lengths)] = np.array(teller_id_lengths)\n",
    "        stacked_drawer_turn_ids_lengths[i, :len(drawer_id_lengths)] = np.array(drawer_id_lengths)\n",
    "        stacked_teller_drawer_turn_ids_lengths[\n",
    "            i, :len(teller_drawer_id_lengths)] = np.array(teller_drawer_id_lengths)\n",
    "        \n",
    "        for j, turn in enumerate(turns_word_embedding):\n",
    "            turn_len = turns_lengths[j]\n",
    "            stacked_turns[i, j, :turn_len] = turn[:turn_len]\n",
    "        \n",
    "        #Update the stacked teller_turns\n",
    "        for j, teller_turn_id in enumerate(teller_turn_ids):\n",
    "            teller_id_len = teller_id_lengths[j]\n",
    "            stacked_teller_turn_ids[i,j,:teller_id_len] = np.array(teller_turn_id)\n",
    "            \n",
    "        for j, drawer_turn_id in enumerate(drawer_turn_ids):\n",
    "            drawer_id_len = drawer_id_lengths[j]\n",
    "            stacked_drawer_turn_ids[i,j,:drawer_id_len] = np.array(drawer_turn_id)\n",
    "        \n",
    "        for j, teller_drawer_turn_id in enumerate(teller_drawer_turn_ids):\n",
    "            teller_drawer_id_len = teller_drawer_id_lengths[j]\n",
    "            stacked_teller_drawer_turn_ids[\n",
    "                i, j, :teller_drawer_id_len] = np.array(teller_drawer_turn_id)\n",
    "\n",
    "    sample = {\n",
    "        'scene_id': np.array(scene_ids),\n",
    "        'image': torch.FloatTensor(stacked_images),\n",
    "        'turn': np.array(turns_text),\n",
    "        'turn_word_embedding': torch.FloatTensor(stacked_turns),\n",
    "        'turn_lengths': torch.LongTensor(stacked_turn_lengths),\n",
    "        'dialog_length': torch.LongTensor(np.array(dialog_lengths)),\n",
    "        'background': torch.FloatTensor(background),\n",
    "        'objects': torch.FloatTensor(stacked_objects),\n",
    "        #Include the additional item\n",
    "        'target_image': torch.FloatTensor(stacked_target_images),\n",
    "        'target_image_segmentation': target_image_segmentation,\n",
    "        'teller_turn_ids': torch.Tensor(stacked_teller_turn_ids),\n",
    "        'drawer_turn_ids': torch.Tensor(stacked_drawer_turn_ids),\n",
    "        'teller_drawer_turn_ids': torch.LongTensor(stacked_teller_drawer_turn_ids),\n",
    "        'teller_id_lengths': torch.LongTensor(stacked_teller_turn_ids_lengths),\n",
    "        'drawer_id_lengths': torch.LongTensor(stacked_drawer_turn_ids_lengths),\n",
    "        'teller_drawer_id_lengths': torch.LongTensor(stacked_teller_drawer_turn_ids_lengths)\n",
    "    }\n",
    "\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(cfg)\n",
    "shuffle = False\n",
    "gandraw_dataloader = DataLoader(gandraw_dataset,\n",
    "                                     batch_size=cfg.batch_size,\n",
    "                                     shuffle=shuffle,\n",
    "                                     num_workers=cfg.num_workers,\n",
    "                                     pin_memory=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "gandraw_dataloader.collate_fn = gandraw_collate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop the Model and Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-30389c200f29>\", line 119, in __getitem__\n    images = example['images'].value\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/h5py/_hl/group.py\", line 177, in __getitem__\n    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5o.pyx\", line 190, in h5py.h5o.open\nKeyError: 'Unable to open object (wrong B-tree signature)'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-da84d4fbac6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#codrawDialog_dataset.shuffle()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgandraw_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(images.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geneva/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geneva/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geneva/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-30389c200f29>\", line 119, in __getitem__\n    images = example['images'].value\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/h5py/_hl/group.py\", line 177, in __getitem__\n    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5o.pyx\", line 190, in h5py.h5o.open\nKeyError: 'Unable to open object (wrong B-tree signature)'\n"
     ]
    }
   ],
   "source": [
    "#codrawDialog_dataset.shuffle()\n",
    "for batch in gandraw_dataloader:\n",
    "    images = batch['image']\n",
    "    #print(images.size())\n",
    "    break\n",
    "# import numpy as np\n",
    "\n",
    "# hdf5_dataset = h5py.File(trainer.dataset.dataset_path, 'r')\n",
    "# hdf5_dataset_blocks_maps = trainer.dataset.blocks_maps\n",
    "# #Random Select an Example\n",
    "# example = hdf5_dataset[str(hdf5_dataset_blocks_maps[0][0])]\n",
    "# example_images = example[\"images\"].value\n",
    "# print(example_images.shape)\n",
    "\n",
    "# images = example_images[..., ::-1] #I don't quite understand this performance\n",
    "# images = images / 128. - 1\n",
    "# images += np.random.uniform(size=images.shape, low=0, high=1. / 64)\n",
    "# images = images.transpose(0, 3, 1, 2)\n",
    "# print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(codrawDialog_dataloader):\n",
    "#     teller_ids = batch[\"teller_turn_ids\"]\n",
    "#     drawer_ids = batch[\"drawer_turn_ids\"]\n",
    "#     #teller_drawer_ids = batch[\"teller_drawer_turn_ids\"]\n",
    "#     teller_id_lengths = batch[\"teller_id_lengths\"]\n",
    "#     print(teller_ids[0])\n",
    "#     print(drawer_ids[0])\n",
    "#     #print(teller_drawer_ids[0])\n",
    "#     break\n",
    "\n",
    "# block_index=0\n",
    "# sample_index=0\n",
    "# example = hdf5_dataset[str(dataset.blocks_maps[block_index][sample_index])]\n",
    "# teller_utterances = example[\"utterences\"].value\n",
    "# for x in example:\n",
    "#     print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_example.keys())\n",
    "# print(\"turn_lengths is: {}\".format(batch_example[\"turn_lengths\"]))  \n",
    "print(\"dialog_length is: {}\".format(batch_example[\"dialog_length\"]))\n",
    "# print(\"turn is: {}\".format(batch_example[\"turn\"]))\n",
    "# print(\"image is: {}\".format(batch_example[\"image\"]))\n",
    "image = batch_example[\"image\"]\n",
    "print(image.shape) #shape: (batch_size, max_dialog_length, 3 colored channel, )\n",
    "scene_id = batch_example[\"scene_id\"]\n",
    "objects = batch_example['objects'] #Identify the objects for each turn \n",
    "entities = batch_example['entities'] #Specify the list of the objects\n",
    "#print(\"objects are: {}\".format(objects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Dataloader\n",
    "In the new dataloader for our teller, on top of the existing data, we should include\n",
    "(1) teller_turn: The utterance from teller alone. \n",
    "(2) teller_turn_lentghs: The tokens for each utterance of teller in the dialog\n",
    "(3) target_image: The tensor for the target image \n",
    "(4) modified_objects: The tensor that specify the changed objects at every turn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
