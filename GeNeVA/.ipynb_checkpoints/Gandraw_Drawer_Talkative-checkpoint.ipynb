{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json  # Added to initialize the setting in Jupyter Notebook-by Mingyang\n",
    "import easydict  # Added to initialize the setting in Jupyter Notebook-by Mingyang\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from PIL import Image\n",
    "\n",
    "from geneva.models.models import INFERENCE_MODELS\n",
    "from geneva.data.datasets import DATASETS\n",
    "from geneva.evaluation.evaluate import Evaluator\n",
    "from geneva.utils.config import keys, parse_config\n",
    "from geneva.utils.visualize import VisdomPlotter\n",
    "from geneva.models.models import MODELS\n",
    "from geneva.data import codraw_dataset\n",
    "from geneva.data import clevr_dataset\n",
    "from geneva.data import gandraw_dataset\n",
    "from geneva.evaluation.seg_scene_similarity_score import report_gandraw_eval_result\n",
    "from geneva.utils.config import keys\n",
    "from math import sqrt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_glove(glove_path):\n",
    "    glove = {}\n",
    "    with open(glove_path, 'r') as f:\n",
    "        for line in f:\n",
    "            splitline = line.split()\n",
    "            word = splitline[0]\n",
    "            embedding = np.array([float(val) for val in splitline[1:]])\n",
    "            glove[word] = embedding\n",
    "\n",
    "    return glove\n",
    "\n",
    "class GanDraw_Talkative_Drawer():\n",
    "    def __init__(self, cfg, pretrained_model_path=None, iteration=1000):\n",
    "        self.cfg = cfg\n",
    "        #Load the Dataset\n",
    "        dataset_path = cfg.test_dataset\n",
    "        gandraw_vocab_path = \"/home/zmykevin/CoDraw_Gaugan/data/GanDraw/data_full_filtered/gandraw_vocab.txt\"\n",
    "        with open(gandraw_vocab_path, 'r') as f:\n",
    "            gandraw_vocab = f.readlines()\n",
    "            gandraw_vocab = [x.strip().rsplit(' ', 1)[0] for x in gandraw_vocab]        \n",
    "        self.vocab = ['<s_start>', '<s_end>', '<unk>', '<pad>', '<d_end>'] + gandraw_vocab        \n",
    "        self.cfg.vocab_size = len(self.vocab)\n",
    "        \n",
    "        self.model = INFERENCE_MODELS[self.cfg.gan_type](self.cfg)\n",
    "        #load the pretrained_model\n",
    "        if pretrained_model_path is not None:\n",
    "            self.model.load(pretrained_model_path,iteration)\n",
    "            \n",
    "        self.current_iteration=iteration\n",
    "        \n",
    "        \n",
    "        self.visualize_batch = 0\n",
    "        # Keep all the progress images to be processed.\n",
    "        self.visualize_images = []\n",
    "        \n",
    "        self.default_drawer_utt = [\"okay\", \"done\", \"next\"]\n",
    "        self.glove = _parse_glove(keys['glove_gandraw_path'])\n",
    "        self.unk_embedding = np.load(\"unk_embedding.npy\")\n",
    "        self.get_background_embedding()\n",
    "        \n",
    "        self.word2index = {k: v for v, k in enumerate(self.vocab)}\n",
    "        self.index2word = {v: k for v, k in enumerate(self.vocab)}\n",
    "        self.prev_utt = None\n",
    "    def get_background_embedding(self):\n",
    "        self.background_embedding = np.zeros((self.cfg.img_size, self.cfg.img_size, 22), dtype=np.int32)\n",
    "        self.background_embedding[:,:,0] = 1 #Define the background with sky label activated\n",
    "        self.background_embedding =  np.expand_dims(self.background_embedding, axis=0)\n",
    "        self.background_embedding = self.process_image(self.background_embedding)\n",
    "        self.background_embedding = torch.FloatTensor(self.background_embedding)\n",
    "        \n",
    "    def generate_im_utt(self, input_text):\n",
    "        #TODO: build the function to generate_im\n",
    "        with torch.no_grad():\n",
    "            current_turn_embedding, current_turn_len = self.utt2embedding(input_text)\n",
    "            #Get the user ids as well\n",
    "            input_utt_ids, input_utt_ids_len = self.utt2ids(input_text)\n",
    "            gen_im, output_utt = self.model.generate_im_utt(current_turn_embedding, current_turn_len, input_utt_ids, input_utt_ids_len, self.word2index, self.index2word)\n",
    "            self.prev_utt = output_utt\n",
    "            gen_im = self.post_processing_im(gen_im)\n",
    "        return gen_im, output_utt\n",
    "    \n",
    "    def utt2ids(self, input_text):\n",
    "        #Tokenize the input_text\n",
    "        if self.prev_utt is not None:\n",
    "            drawer_text_tokens = ['<drawer>'] + nltk.word_tokenize(self.prev_utt)\n",
    "        else:\n",
    "            drawer_text_tokens = []\n",
    "        teller_text_tokens = ['<teller'] + nltk.word_tokenize(input_text)\n",
    "        all_tokens = drawer_text_tokens + teller_text_tokens\n",
    "        all_tokens_ids = [0] + [self.word2index.get(x, self.word2index['<unk>']) for x in all_tokens if x!= \"<teller>\" and x!= \"<drawer>\"]+[1]\n",
    "        all_tokens_len = len(all_tokens_ids)\n",
    "        \n",
    "        turn_teller_drawer_ids = np.array(all_tokens_ids)\n",
    "        turn_teller_drawer_ids = np.expand_dims(turn_teller_drawer_ids, axis=0)\n",
    "        turn_teller_drawer_ids_len = np.ones((1))*all_tokens_len\n",
    "        \n",
    "        return torch.LongTensor(turn_teller_drawer_ids), torch.LongTensor(turn_teller_drawer_ids_len)\n",
    "    \n",
    "    def utt2embedding(self, input_text):\n",
    "        #Tokenize the input_text\n",
    "        text_tokens = ['<teller>'] + nltk.word_tokenize(input_text)\n",
    "        #sampled_drawer_utt = ['<drawer>']+nltk.word_tokenize(random.choice(self.default_drawer_utt))\n",
    "        if self.prev_utt is not None:\n",
    "            sampled_drawer_utt = ['<drawer>'] + nltk.word_tokenize(self.prev_utt)\n",
    "        else:\n",
    "            sampled_drawer_utt = []\n",
    "        text_tokens = sampled_drawer_utt + text_tokens\n",
    "        #get padded_input_text\n",
    "        processed_text_tokens =  [w for w in text_tokens if w not in string.punctuation]\n",
    "        processed_text_len = len(processed_text_tokens)\n",
    "        #initialize turn embedding \n",
    "        turn_embeddings = np.zeros((processed_text_len, 300))\n",
    "        for i,w in enumerate(processed_text_tokens):\n",
    "            turn_embeddings[i] = self.glove.get(w, self.unk_embedding)\n",
    "        #turns_embeddings is not a numpy matrix\n",
    "        turn_embeddings = np.expand_dims(turn_embeddings, axis=0)\n",
    "        turn_lens = np.ones((1))*processed_text_len\n",
    "        \n",
    "        return torch.FloatTensor(turn_embeddings), torch.LongTensor(turn_lens)\n",
    "    def reset_drawer(self):\n",
    "        self.model.reset_drawer(self.background_embedding)\n",
    "        self.prev_utt = None\n",
    "        #self.model.eval()\n",
    "    def post_processing_im(self, gen_im,resize_wh=512):\n",
    "        dominant_label = np.unique(gen_im)\n",
    "        output_image = cv2.resize(gen_im, (resize_wh, resize_wh), interpolation=cv2.INTER_AREA)\n",
    "        output_image = self.smooth_segmentation(output_image, dominant_label)\n",
    "        return output_image\n",
    "    def smooth_segmentation(self, image, dominant_label):\n",
    "        \"\"\"\n",
    "        image is the 3D gray scale image with each pixel equal to the label of a certain category.\n",
    "        return the same size of shrinked_image with only dominant_label\n",
    "        \"\"\"\n",
    "        drawing2landscape = [\n",
    "            ([0, 0, 0],156), #sky\n",
    "            ([156, 156, 156], 156),#sky\n",
    "            ([154, 154, 154], 154), #sea\n",
    "            ([134, 134, 134], 134), #mountain\n",
    "            ([149, 149, 149], 149), #rock\n",
    "            ([126, 126, 126], 126), #hill\n",
    "            ([105, 105, 105], 105), #clouds\n",
    "            ([14, 14, 14], 14), #sand\n",
    "            ([124, 124, 124], 124), #gravel\n",
    "            ([158, 158, 158], 158), #snow\n",
    "            ([147, 147, 147], 147), #river\n",
    "            ([96, 96, 96], 96), #bush\n",
    "            ([168, 168, 168], 168), #tree\n",
    "            ([148, 148, 148], 148), #road\n",
    "            ([110, 110, 110], 110), #dirt \n",
    "            ([135, 135, 135], 135), #mud \n",
    "            ([119, 119, 119], 119), #fog \n",
    "            ([161, 161, 161], 161), #stone\n",
    "            ([177, 177, 177], 177), #water\n",
    "            ([118, 118, 118], 118), #flower\n",
    "            ([123, 123, 123], 123), #grass\n",
    "            ([162, 162, 162], 162), #straw\n",
    "        ]\n",
    "\n",
    "        center = []\n",
    "        for l in dominant_label:\n",
    "            center_array = np.array([l]*3)\n",
    "            center.append(np.uint8(center_array))\n",
    "        #print(center)\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                current_pixel = np.uint8(image[i,j])\n",
    "                #sort centers\n",
    "\n",
    "                if not any(all(current_pixel == x) for x in center):\n",
    "                    #print(\"sort_center\")\n",
    "                    center.sort(key=lambda c: sqrt((current_pixel[0]-c[0])**2+(current_pixel[1]-c[1])**2+(current_pixel[2]-c[2])**2))\n",
    "                    image[i,j] = center[0]\n",
    "        #print(image)\n",
    "        return image\n",
    "    def process_image(self, images):\n",
    "        if self.cfg.image_gen_mode == \"real\":\n",
    "            result_images = np.zeros_like(\n",
    "                images.transpose(0, 3, 1, 2), dtype=np.float32)\n",
    "            for i in range(images.shape[0]):\n",
    "                current_img = images[i]\n",
    "                current_processed_img = self.image_transform(current_img)\n",
    "                current_processed_img = current_processed_img.numpy()\n",
    "                result_images[i] = current_processed_img\n",
    "        \n",
    "        elif self.cfg.image_gen_mode == \"segmentation\":\n",
    "            result_images = images[..., ::-1]\n",
    "            #print(result_images.shape)\n",
    "            result_images = result_images / 128. - 1\n",
    "            result_images += np.random.uniform(size=result_images.shape, low=0, high=1. / 64)\n",
    "            result_images = result_images.transpose(0, 3, 1, 2)\n",
    "        elif self.cfg.image_gen_mode == \"segmentation_onehot\":\n",
    "            #We don't preprocess the image in this setting, switch the channel to the second dimension\n",
    "            result_images = images.transpose(0,3,1,2)\n",
    "        return result_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"example_args/gandraw_drawer_args.json\"\n",
    "# Load the config_file\n",
    "with open(config_file, 'r') as f:\n",
    "    cfg = json.load(f)\n",
    "# convert cfg as easydict\n",
    "cfg = easydict.EasyDict(cfg)\n",
    "cfg.load_snapshot = None\n",
    "#Fix the seed\n",
    "n_gpu = torch.cuda.device_count()\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "drawer = GanDraw_Talkative_Drawer(cfg, pretrained_model_path=\"logs/gandraw/drawer/gandraw_drawer_lr0.00001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the drawer\n",
    "drawer.reset_drawer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate images\n",
    "sample_dialog = [\n",
    "                {\n",
    "                    \"drawer\": \"what next\",\n",
    "                    \"drawer_label\": \"Elicit_Information\",\n",
    "                    \"image_semantic\": \"image_data/3b224bb2.png\",\n",
    "                    \"image_synthetic\": \"image_data/cd4ee72e.jpg\",\n",
    "                    \"teller\": \"very bottom is grass one sixth of image. then small line of trees\",\n",
    "                    \"teller_label\": \"Describe_Image\"\n",
    "                },\n",
    "                {\n",
    "                    \"drawer\": \"like this?\",\n",
    "                    \"drawer_label\": \"Request_Correction\",\n",
    "                    \"image_semantic\": \"image_data/88f04886.png\",\n",
    "                    \"image_synthetic\": \"image_data/2d147646.jpg\",\n",
    "                    \"teller\": \"higher trees on the right\",\n",
    "                    \"teller_label\": \"Describe_Image\"\n",
    "                },\n",
    "                {\n",
    "                    \"drawer\": \"thanks\",\n",
    "                    \"drawer_label\": \"Other\",\n",
    "                    \"image_semantic\": \"image_data/11530ed4.png\",\n",
    "                    \"image_synthetic\": \"image_data/3f37b13c.jpg\",\n",
    "                    \"teller\": \"yes very good\",\n",
    "                    \"teller_label\": \"Other\"\n",
    "                }\n",
    "            ]\n",
    "sample_output_path = \"gandraw_drawer_result\"\n",
    "for i,turn in enumerate(sample_dialog):\n",
    "    input_text = turn[\"teller\"]\n",
    "    output_img, output_utt = drawer.generate_im_utt(input_text)\n",
    "    print(output_utt)\n",
    "    output_img = Image.fromarray(np.uint8(output_img))\n",
    "    output_img.save('/'.join([sample_output_path,\"test_img_talktive_{}.png\".format(i)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
