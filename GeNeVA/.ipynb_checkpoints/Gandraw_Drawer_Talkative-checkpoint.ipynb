{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmykevin/CoDraw_Gaugan/code/GanDraw/GeNeVA/geneva/utils/config.py:15: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  keys = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json  # Added to initialize the setting in Jupyter Notebook-by Mingyang\n",
    "import easydict  # Added to initialize the setting in Jupyter Notebook-by Mingyang\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from PIL import Image\n",
    "\n",
    "from geneva.models.models import INFERENCE_MODELS\n",
    "from geneva.data.datasets import DATASETS\n",
    "from geneva.evaluation.evaluate import Evaluator\n",
    "from geneva.utils.config import keys, parse_config\n",
    "from geneva.utils.visualize import VisdomPlotter\n",
    "from geneva.models.models import MODELS\n",
    "from geneva.data import codraw_dataset\n",
    "from geneva.data import clevr_dataset\n",
    "from geneva.data import gandraw_dataset\n",
    "from geneva.evaluation.seg_scene_similarity_score import report_gandraw_eval_result\n",
    "from geneva.utils.config import keys\n",
    "from math import sqrt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_glove(glove_path):\n",
    "    glove = {}\n",
    "    with open(glove_path, 'r') as f:\n",
    "        for line in f:\n",
    "            splitline = line.split()\n",
    "            word = splitline[0]\n",
    "            embedding = np.array([float(val) for val in splitline[1:]])\n",
    "            glove[word] = embedding\n",
    "\n",
    "    return glove\n",
    "\n",
    "class GanDraw_Talkative_Drawer():\n",
    "    def __init__(self, cfg, pretrained_model_path=None, iteration=1000):\n",
    "        self.cfg = cfg\n",
    "        #Load the Dataset\n",
    "        dataset_path = cfg.test_dataset\n",
    "        gandraw_vocab_path = \"/home/zmykevin/CoDraw_Gaugan/data/GanDraw/data_full_filtered/gandraw_vocab.txt\"\n",
    "        with open(gandraw_vocab_path, 'r') as f:\n",
    "            gandraw_vocab = f.readlines()\n",
    "            gandraw_vocab = [x.strip().rsplit(' ', 1)[0] for x in gandraw_vocab]        \n",
    "        self.vocab = ['<s_start>', '<s_end>', '<unk>', '<pad>', '<d_end>'] + gandraw_vocab        \n",
    "        self.cfg.vocab_size = len(self.vocab)\n",
    "        \n",
    "        self.model = INFERENCE_MODELS[self.cfg.gan_type](self.cfg)\n",
    "        #load the pretrained_model\n",
    "        if pretrained_model_path is not None:\n",
    "            self.model.load(pretrained_model_path,iteration)\n",
    "            \n",
    "        self.current_iteration=iteration\n",
    "        \n",
    "        \n",
    "        self.visualize_batch = 0\n",
    "        # Keep all the progress images to be processed.\n",
    "        self.visualize_images = []\n",
    "        \n",
    "        self.default_drawer_utt = [\"okay\", \"done\", \"next\"]\n",
    "        self.glove = _parse_glove(keys['glove_gandraw_path'])\n",
    "        self.unk_embedding = np.load(\"unk_embedding.npy\")\n",
    "        self.get_background_embedding()\n",
    "        \n",
    "        self.word2index = {k: v for v, k in enumerate(self.vocab)}\n",
    "        self.index2word = {v: k for v, k in enumerate(self.vocab)}\n",
    "        self.prev_utt = None\n",
    "    def get_background_embedding(self):\n",
    "        self.background_embedding = np.zeros((self.cfg.img_size, self.cfg.img_size, 22), dtype=np.int32)\n",
    "        self.background_embedding[:,:,0] = 1 #Define the background with sky label activated\n",
    "        self.background_embedding =  np.expand_dims(self.background_embedding, axis=0)\n",
    "        self.background_embedding = self.process_image(self.background_embedding)\n",
    "        self.background_embedding = torch.FloatTensor(self.background_embedding)\n",
    "        \n",
    "    def generate_im_utt(self, input_text):\n",
    "        #TODO: build the function to generate_im\n",
    "        with torch.no_grad():\n",
    "            current_turn_embedding, current_turn_len = self.utt2embedding(input_text)\n",
    "            #Get the user ids as well\n",
    "            input_utt_ids, input_utt_ids_len = self.utt2ids(input_text)\n",
    "            gen_im, output_utt = self.model.generate_im_utt(current_turn_embedding, current_turn_len, input_utt_ids, input_utt_ids_len, self.word2index, self.index2word)\n",
    "            self.prev_utt = output_utt\n",
    "            gen_im = self.post_processing_im(gen_im)\n",
    "        return gen_im, output_utt\n",
    "    \n",
    "    def utt2ids(self, input_text):\n",
    "        #Tokenize the input_text\n",
    "        if self.prev_utt is not None:\n",
    "            drawer_text_tokens = ['<drawer>'] + nltk.word_tokenize(self.prev_utt)\n",
    "        else:\n",
    "            drawer_text_tokens = []\n",
    "        teller_text_tokens = ['<teller'] + nltk.word_tokenize(input_text)\n",
    "        all_tokens = drawer_text_tokens + teller_text_tokens\n",
    "        all_tokens_ids = [0] + [self.word2index.get(x, self.word2index['<unk>']) for x in all_tokens if x!= \"<teller>\" and x!= \"<drawer>\"]+[1]\n",
    "        all_tokens_len = len(all_tokens_ids)\n",
    "        \n",
    "        turn_teller_drawer_ids = np.array(all_tokens_ids)\n",
    "        turn_teller_drawer_ids = np.expand_dims(turn_teller_drawer_ids, axis=0)\n",
    "        turn_teller_drawer_ids_len = np.ones((1))*all_tokens_len\n",
    "        \n",
    "        return torch.LongTensor(turn_teller_drawer_ids), torch.LongTensor(turn_teller_drawer_ids_len)\n",
    "    \n",
    "    def utt2embedding(self, input_text):\n",
    "        #Tokenize the input_text\n",
    "        text_tokens = ['<teller>'] + nltk.word_tokenize(input_text)\n",
    "        #sampled_drawer_utt = ['<drawer>']+nltk.word_tokenize(random.choice(self.default_drawer_utt))\n",
    "        if self.prev_utt is not None:\n",
    "            sampled_drawer_utt = ['<drawer>'] + nltk.word_tokenize(self.prev_utt)\n",
    "        else:\n",
    "            sampled_drawer_utt = []\n",
    "        text_tokens = sampled_drawer_utt + text_tokens\n",
    "        #get padded_input_text\n",
    "        processed_text_tokens =  [w for w in text_tokens if w not in string.punctuation]\n",
    "        processed_text_len = len(processed_text_tokens)\n",
    "        #initialize turn embedding \n",
    "        turn_embeddings = np.zeros((processed_text_len, 300))\n",
    "        for i,w in enumerate(processed_text_tokens):\n",
    "            turn_embeddings[i] = self.glove.get(w, self.unk_embedding)\n",
    "        #turns_embeddings is not a numpy matrix\n",
    "        turn_embeddings = np.expand_dims(turn_embeddings, axis=0)\n",
    "        turn_lens = np.ones((1))*processed_text_len\n",
    "        \n",
    "        return torch.FloatTensor(turn_embeddings), torch.LongTensor(turn_lens)\n",
    "    def reset_drawer(self):\n",
    "        self.model.reset_drawer(self.background_embedding)\n",
    "        self.prev_utt = None\n",
    "        #self.model.eval()\n",
    "    def post_processing_im(self, gen_im,resize_wh=512):\n",
    "        dominant_label = np.unique(gen_im)\n",
    "        output_image = cv2.resize(gen_im, (resize_wh, resize_wh), interpolation=cv2.INTER_AREA)\n",
    "        output_image = self.smooth_segmentation(output_image, dominant_label)\n",
    "        return output_image\n",
    "    def smooth_segmentation(self, image, dominant_label):\n",
    "        \"\"\"\n",
    "        image is the 3D gray scale image with each pixel equal to the label of a certain category.\n",
    "        return the same size of shrinked_image with only dominant_label\n",
    "        \"\"\"\n",
    "        drawing2landscape = [\n",
    "            ([0, 0, 0],156), #sky\n",
    "            ([156, 156, 156], 156),#sky\n",
    "            ([154, 154, 154], 154), #sea\n",
    "            ([134, 134, 134], 134), #mountain\n",
    "            ([149, 149, 149], 149), #rock\n",
    "            ([126, 126, 126], 126), #hill\n",
    "            ([105, 105, 105], 105), #clouds\n",
    "            ([14, 14, 14], 14), #sand\n",
    "            ([124, 124, 124], 124), #gravel\n",
    "            ([158, 158, 158], 158), #snow\n",
    "            ([147, 147, 147], 147), #river\n",
    "            ([96, 96, 96], 96), #bush\n",
    "            ([168, 168, 168], 168), #tree\n",
    "            ([148, 148, 148], 148), #road\n",
    "            ([110, 110, 110], 110), #dirt \n",
    "            ([135, 135, 135], 135), #mud \n",
    "            ([119, 119, 119], 119), #fog \n",
    "            ([161, 161, 161], 161), #stone\n",
    "            ([177, 177, 177], 177), #water\n",
    "            ([118, 118, 118], 118), #flower\n",
    "            ([123, 123, 123], 123), #grass\n",
    "            ([162, 162, 162], 162), #straw\n",
    "        ]\n",
    "\n",
    "        center = []\n",
    "        for l in dominant_label:\n",
    "            center_array = np.array([l]*3)\n",
    "            center.append(np.uint8(center_array))\n",
    "        #print(center)\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                current_pixel = np.uint8(image[i,j])\n",
    "                #sort centers\n",
    "\n",
    "                if not any(all(current_pixel == x) for x in center):\n",
    "                    #print(\"sort_center\")\n",
    "                    center.sort(key=lambda c: sqrt((current_pixel[0]-c[0])**2+(current_pixel[1]-c[1])**2+(current_pixel[2]-c[2])**2))\n",
    "                    image[i,j] = center[0]\n",
    "        #print(image)\n",
    "        return image\n",
    "    def process_image(self, images):\n",
    "        if self.cfg.image_gen_mode == \"real\":\n",
    "            result_images = np.zeros_like(\n",
    "                images.transpose(0, 3, 1, 2), dtype=np.float32)\n",
    "            for i in range(images.shape[0]):\n",
    "                current_img = images[i]\n",
    "                current_processed_img = self.image_transform(current_img)\n",
    "                current_processed_img = current_processed_img.numpy()\n",
    "                result_images[i] = current_processed_img\n",
    "        \n",
    "        elif self.cfg.image_gen_mode == \"segmentation\":\n",
    "            result_images = images[..., ::-1]\n",
    "            #print(result_images.shape)\n",
    "            result_images = result_images / 128. - 1\n",
    "            result_images += np.random.uniform(size=result_images.shape, low=0, high=1. / 64)\n",
    "            result_images = result_images.transpose(0, 3, 1, 2)\n",
    "        elif self.cfg.image_gen_mode == \"segmentation_onehot\":\n",
    "            #We don't preprocess the image in this setting, switch the channel to the second dimension\n",
    "            result_images = images.transpose(0,3,1,2)\n",
    "        return result_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"example_args/gandraw_drawer_args.json\"\n",
    "# Load the config_file\n",
    "with open(config_file, 'r') as f:\n",
    "    cfg = json.load(f)\n",
    "# convert cfg as easydict\n",
    "cfg = easydict.EasyDict(cfg)\n",
    "cfg.load_snapshot = None\n",
    "#Fix the seed\n",
    "n_gpu = torch.cuda.device_count()\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "drawer = GanDraw_Talkative_Drawer(cfg, pretrained_model_path=\"logs/gandraw/drawer/gandraw_drawer_lr0.001\", iteration=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from uuid import uuid4\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import StringIO, BytesIO\n",
    "import os\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def segmap_to_real(seg_map, style=\"1\"):\n",
    "    \"\"\"\n",
    "    Give a \"seg map\", calls the API and converts it to a realistic image\n",
    "    \"\"\"\n",
    "    urls = ['http://54.191.227.231:443/', 'http://34.221.84.127:443/', 'http://34.216.59.35:443/']\n",
    "    request_url = urls[randint(0, 2)] + 'nvidia_gaugan_submit_map'\n",
    "    unique_request = str(uuid4())[:8]\n",
    "    data = {'imageBase64':seg_map, 'name':unique_request}\n",
    "    try:\n",
    "        result = requests.post(request_url, data = data, timeout=2)\n",
    "    except:\n",
    "        return (Image.new('RGB', (512, 512)), False)\n",
    "    if result.status_code != 200:\n",
    "        print(\"Error submitting image to GauGan API!\")\n",
    "        return (Image.new('RGB', (512, 512)), False)\n",
    "    data = {}\n",
    "    request_url = request_url.replace(\"nvidia_gaugan_submit_map\", \"nvidia_gaugan_receive_image\")    \n",
    "    try:\n",
    "        r = requests.post(request_url, data = {'name':unique_request, \"style_name\":style}, timeout=5)    \n",
    "    except:\n",
    "        return (Image.new('RGB', (512, 512)), False)\n",
    "    if r.status_code != 200:\n",
    "        print(\"Error submitting image to GauGan API!\")\n",
    "        return (Image.new('RGB', (512, 512)), False)\n",
    "    bytes_data = BytesIO(r.content)\n",
    "    img = Image.open(bytes_data)\n",
    "    return (img, True)\n",
    "\n",
    "def img_to_bytes(image, intermediate=\"target_images\"):\n",
    "    #path_to_try = f\"{os.getcwd()}/{intermediate}/{path}\"\n",
    "    try:\n",
    "        #image = Image.open(path_to_try)\n",
    "        imgByteArr = io.BytesIO()\n",
    "        image.save(imgByteArr, format='PNG')        \n",
    "        return 'data:image/png;base64,'+ base64.b64encode(imgByteArr.getvalue()).decode('ascii')\n",
    "    except:\n",
    "        print(path_to_try)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the drawer\n",
    "drawer.reset_drawer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got it\n",
      "data:image/png;base64,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\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0adc1d31cac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mreturned_synthetic_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegmap_to_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_be64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcorrect_return\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturned_synthetic_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "#generate images\n",
    "sample_dialog = [\n",
    "                {\n",
    "                    \"drawer\": \"what next\",\n",
    "                    \"drawer_label\": \"Elicit_Information\",\n",
    "                    \"image_semantic\": \"image_data/3b224bb2.png\",\n",
    "                    \"image_synthetic\": \"image_data/cd4ee72e.jpg\",\n",
    "                    \"teller\": \"very bottom is grass one sixth of image. then small line of trees\",\n",
    "                    \"teller_label\": \"Describe_Image\"\n",
    "                },\n",
    "                {\n",
    "                    \"drawer\": \"like this?\",\n",
    "                    \"drawer_label\": \"Request_Correction\",\n",
    "                    \"image_semantic\": \"image_data/88f04886.png\",\n",
    "                    \"image_synthetic\": \"image_data/2d147646.jpg\",\n",
    "                    \"teller\": \"higher trees on the right\",\n",
    "                    \"teller_label\": \"Describe_Image\"\n",
    "                },\n",
    "                {\n",
    "                    \"drawer\": \"thanks\",\n",
    "                    \"drawer_label\": \"Other\",\n",
    "                    \"image_semantic\": \"image_data/11530ed4.png\",\n",
    "                    \"image_synthetic\": \"image_data/3f37b13c.jpg\",\n",
    "                    \"teller\": \"yes very good\",\n",
    "                    \"teller_label\": \"Other\"\n",
    "                }\n",
    "            ]\n",
    "sample_output_path = \"gandraw_drawer_result\"\n",
    "for i,turn in enumerate(sample_dialog):\n",
    "    input_text = turn[\"teller\"]\n",
    "    output_img, output_utt = drawer.generate_im_utt(input_text)\n",
    "    print(output_utt)\n",
    "    \n",
    "    #Convert output_img to ByteString\n",
    "    output_img = Image.fromarray(np.uint8(output_img))\n",
    "    \n",
    "    #convert output_img to ByteString\n",
    "    output_be64 = img_to_bytes(output_img)\n",
    "    #print(output_be64)\n",
    "    #get the synthetic image from GauGAN\n",
    "    returned_synthetic_img, correct_return = segmap_to_real(output_be64)\n",
    "    if correct_return:\n",
    "        plt.imshow(returned_synthetic_img)\n",
    "        plt.show()\n",
    "    break\n",
    "#     output_img.save('/'.join([sample_output_path,\"test_img_talktive_{}.png\".format(i)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
