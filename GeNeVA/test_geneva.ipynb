{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import os\n",
    "import json # Added to initialize the setting in Jupyter Notebook-by Mingyang\n",
    "import easydict # Added to initialize the setting in Jupyter Notebook-by Mingyang\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from geneva.data.datasets import DATASETS\n",
    "from geneva.evaluation.evaluate_metrics import report_inception_objects_score\n",
    "from geneva.utils.config import keys, parse_config\n",
    "from geneva.models.models import INFERENCE_MODELS\n",
    "from geneva.data import codraw_dataset\n",
    "from geneva.data import clevr_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester():\n",
    "    def __init__(self, cfg, use_val=False, iteration=None, test_eval=False):\n",
    "        self.model = INFERENCE_MODELS[cfg.gan_type](cfg)\n",
    "\n",
    "        if use_val:\n",
    "            dataset_path = cfg.val_dataset\n",
    "            model_path = os.path.join(cfg.log_path, cfg.exp_name)\n",
    "        else:\n",
    "            dataset_path = cfg.dataset\n",
    "            model_path = cfg.load_snapshot\n",
    "        if test_eval:\n",
    "            dataset_path = cfg.test_dataset\n",
    "            model_path = cfg.load_snapshot\n",
    "\n",
    "        self.model.load(model_path, iteration)\n",
    "        self.dataset = DATASETS[cfg.dataset](path=keys[dataset_path],\n",
    "                                             cfg=cfg,\n",
    "                                             img_size=cfg.img_size)\n",
    "        self.dataloader = DataLoader(self.dataset,\n",
    "                                     batch_size=cfg.batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=cfg.num_workers,\n",
    "                                     drop_last=True)\n",
    "\n",
    "        self.iterations = len(self.dataset) // cfg.batch_size\n",
    "\n",
    "        if cfg.dataset == 'codraw':\n",
    "            self.dataloader.collate_fn = codraw_dataset.collate_data\n",
    "        elif cfg.dataset == 'iclevr':\n",
    "            self.dataloader.collate_fn = clevr_dataset.collate_data\n",
    "\n",
    "        if cfg.results_path is None:\n",
    "            cfg.results_path = os.path.join(cfg.log_path, cfg.exp_name,\n",
    "                                            'results')\n",
    "            if not os.path.exists(cfg.results_path):\n",
    "                os.mkdir(cfg.results_path)\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "    def test(self):\n",
    "        for batch in tqdm(self.dataloader, total=self.iterations):\n",
    "            self.model.predict(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish setting up a tester\n"
     ]
    }
   ],
   "source": [
    "config_file = \"example_args/codraw_args.json\"\n",
    "#Load the config_file\n",
    "with open(config_file, 'r') as f:\n",
    "    cfg = json.load(f)\n",
    "#convert cfg as easydict\n",
    "cfg = easydict.EasyDict(cfg)\n",
    "tester = Tester(cfg, test_eval=True)\n",
    "print(\"finish setting up a tester\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████| 31/31 [00:13<00:00,  4.16it/s]\n"
     ]
    }
   ],
   "source": [
    "tester.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1909/1909 [01:48<00:00, 18.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of images used: 1909\n",
      "JSS: 0.24663185293019207\n",
      " AP: 0.45078372677901224\n",
      "AR: 0.3334648399734832\n",
      " F1: 0.3647282236615615\n",
      "CS: 0.3995373845100403\n",
      "GS: 0.17221260111025694\n",
      "{'jaccard': 0.24663185293019207, 'precision': 0.45078372677901224, 'recall': 0.3334648399734832, 'f1': 0.3647282236615615, 'cossim': 0.39953738, 'relsim': 0.17221260111025694}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/zmykevin/miniconda3/envs/geneva/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "del tester\n",
    "torch.cuda.empty_cache()\n",
    "metrics_report = dict()\n",
    "if cfg.metric_inception_objects:\n",
    "    io_jss, io_ap, io_ar, io_af1, io_cs, io_gs = report_inception_objects_score(None,\n",
    "                                                                                None,\n",
    "                                                                                None,\n",
    "                                                                                cfg.results_path,\n",
    "                                                                                keys[cfg.dataset + '_inception_objects'],\n",
    "                                                                                keys[cfg.test_dataset],\n",
    "                                                                                cfg.dataset)\n",
    "\n",
    "    metrics_report['jaccard'] = io_jss\n",
    "    metrics_report['precision'] = io_ap\n",
    "    metrics_report['recall'] = io_ar\n",
    "    metrics_report['f1'] = io_af1\n",
    "    metrics_report['cossim'] = io_cs\n",
    "    metrics_report['relsim'] = io_gs\n",
    "print(metrics_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
